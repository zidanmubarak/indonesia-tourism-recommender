# -*- coding: utf-8 -*-
"""sistem_rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OS8RyFHut4nQnHnDJaD_EJPajo_U25Sm

# Sistem Rekomendasi Tempat Wisata Indonesia
"""

#!pip install numpy==1.26.4
#!pip install -U surprise

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split as surprise_train_test_split
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

"""# 1. DATA UNDERSTANDING"""

# Load dataset
df = pd.read_csv('tempat_wisata_indonesia.csv')

print("\n1. INFORMASI DATASET")
print("-" * 40)
print(f"Jumlah data: {len(df)} tempat wisata")
print(f"Jumlah kolom: {len(df.columns)} kolom")
print(f"Ukuran dataset: {df.shape}")

print("\n2. INFORMASI KOLOM:")
print("-" * 40)
print(df.info())

print("\n3. STATISTIK DESKRIPTIF:")
print("-" * 40)
print(df.describe())

print("\n4. SAMPLE DATA:")
print("-" * 40)
print(df.head())

print("\n5. KOLOM YANG TERSEDIA:")
print("-" * 40)
for i, col in enumerate(df.columns, 1):
    print(f"{i}. {col}")

"""# 2. UNIVARIATE EXPLORATORY DATA ANALYSIS"""

# Check missing values
print("\n1. MISSING VALUES:")
print("-" * 40)
missing_values = df.isnull().sum()
print(missing_values)

# Check data types
print("\n2. TIPE DATA:")
print("-" * 40)
print(df.dtypes)

# Analyze rating distribution
print("\n3. DISTRIBUSI RATING:")
print("-" * 40)
print(df['rating'].describe())

# Analyze review count
print("\n4. DISTRIBUSI JUMLAH REVIEW:")
print("-" * 40)
print(df['jumlah_review'].describe())

# Analyze provinces
print("\n5. DISTRIBUSI PROVINSI:")
print("-" * 40)
print(df['provinsi'].value_counts())

# Analyze categories
print("\n6. DISTRIBUSI KATEGORI:")
print("-" * 40)
kategori_counts = df['kategori'].str.strip("[]'").str.split("', '").explode().value_counts()
print(kategori_counts)

# 1. Rating distribution
plt.figure(figsize=(8, 6))
plt.hist(df['rating'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribusi Rating Tempat Wisata')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('images/rating_distribution.png')
plt.show()

# 2. Review count distribution
plt.figure(figsize=(8, 6))
plt.hist(df['jumlah_review'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
plt.title('Distribusi Jumlah Review')
plt.xlabel('Jumlah Review')
plt.ylabel('Frekuensi')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('images/review_count_distribution.png')
plt.show()

# 3. Top provinces
top_provinces = df['provinsi'].value_counts().head(10)
plt.figure(figsize=(10, 6))
plt.bar(top_provinces.index, top_provinces.values, color='lightgreen')
plt.title('Top 10 Provinsi dengan Tempat Wisata Terbanyak')
plt.xlabel('Provinsi')
plt.ylabel('Jumlah Tempat Wisata')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('images/province_distribution.png')
plt.show()

# 4. Category distribution
kategori_clean = df['kategori'].str.strip("[]'").str.split("', '").explode()
top_categories = kategori_clean.value_counts().head(10)
plt.figure(figsize=(10, 6))
plt.bar(top_categories.index, top_categories.values, color='plum')
plt.title('Top 10 Kategori Tempat Wisata')
plt.xlabel('Kategori')
plt.ylabel('Jumlah Tempat Wisata')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('images/category_distribution.png')
plt.show()

# 5. Rating vs Review Count scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(df['rating'], df['jumlah_review'], alpha=0.6, color='orange')
plt.title('Hubungan Rating vs Jumlah Review')
plt.xlabel('Rating')
plt.ylabel('Jumlah Review')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('images/rating_vs_review.png')
plt.show()

# 6. Boxplot Rating by Category
kategori_expanded = []
ratings_expanded = []
for idx, row in df.iterrows():
    categories = row['kategori'].strip("[]'").split("', '")
    for cat in categories:
        kategori_expanded.append(cat.strip())
        ratings_expanded.append(row['rating'])

category_rating_df = pd.DataFrame({
    'kategori': kategori_expanded,
    'rating': ratings_expanded
})
top_cats = category_rating_df['kategori'].value_counts().head(5).index
filtered_df = category_rating_df[category_rating_df['kategori'].isin(top_cats)]

plt.figure(figsize=(10, 6))
sns.boxplot(data=filtered_df, x='kategori', y='rating')
plt.title('Distribusi Rating per Kategori (Top 5)')
plt.xlabel('Kategori')
plt.ylabel('Rating')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('images/rating_by_category.png')
plt.show()

"""# 3. DATA PREPROCESSING"""

# Membuat copy dataset untuk preprocessing
df_processed = df.copy()

print("\n1. MEMBERSIHKAN DATA KATEGORI")
print("-" * 40)

# Clean and process categories
def clean_categories(cat_str):
    """Clean category string and return list of categories"""
    if pd.isna(cat_str):
        return ['lainnya']
    # Remove brackets and quotes, split by comma
    cleaned = cat_str.strip("[]'\"").replace("'", "").replace('"', '')
    categories = [cat.strip() for cat in cleaned.split(',')]
    return categories if categories[0] else ['lainnya']

df_processed['kategori_clean'] = df_processed['kategori'].apply(clean_categories)

# Create main category (first category for each place)
df_processed['kategori_utama'] = df_processed['kategori_clean'].apply(lambda x: x[0])

print("Kategori utama setelah pembersihan:")
print(df_processed['kategori_utama'].value_counts())

print("\n2. MENANGANI MISSING VALUES")
print("-" * 40)

# Handle missing values
print("Missing values sebelum cleaning:")
print(df_processed.isnull().sum())

# Drop rows dengan missing values pada kolom penting
df_processed = df_processed.dropna(subset=['alamat', 'rating', 'jumlah_review'])

print("\nMissing values setelah cleaning:")
print(df_processed.isnull().sum())

print("\n3. FEATURE ENGINEERING")
print("-" * 40)

# Create popularity score based on rating and review count
# Normalize review count to 0-5 scale to match rating scale
review_max = df_processed['jumlah_review'].max()
df_processed['review_normalized'] = (df_processed['jumlah_review'] / review_max) * 5

# Calculate popularity score (weighted average of rating and normalized reviews)
df_processed['popularity_score'] = (0.7 * df_processed['rating']) + (0.3 * df_processed['review_normalized'])

print("Popularity score statistics:")
print(df_processed['popularity_score'].describe())

# Create content for content-based filtering
df_processed['content'] = (
    df_processed['nama'] + ' ' +
    df_processed['alamat'] + ' ' +
    df_processed['deskripsi'] + ' ' +
    df_processed['provinsi'] + ' ' +
    df_processed['kategori_utama']
)

print("\n4. DATA SETELAH PREPROCESSING")
print("-" * 40)
print(f"Jumlah data: {len(df_processed)}")
print(f"Kolom baru yang ditambahkan: kategori_clean, kategori_utama, popularity_score, content")

"""# 4. DATA PREPARATION"""

print("\n1. PERSIAPAN DATA UNTUK CONTENT-BASED FILTERING")
print("-" * 40)

# Prepare data for content-based filtering
# Create TF-IDF matrix for content
tfidf_vectorizer = TfidfVectorizer(
    max_features=1000,
    stop_words='english',
    ngram_range=(1, 2),
    min_df=1
)

# Fit and transform the content
tfidf_matrix = tfidf_vectorizer.fit_transform(df_processed['content'])
print(f"TF-IDF Matrix shape: {tfidf_matrix.shape}")

# Calculate cosine similarity matrix
cosine_sim = cosine_similarity(tfidf_matrix)
print(f"Cosine Similarity Matrix shape: {cosine_sim.shape}")

print("\n2. PERSIAPAN DATA UNTUK COLLABORATIVE FILTERING")
print("-" * 40)

# For collaborative filtering, we'll simulate user ratings
# Since we don't have actual user data, we'll create synthetic user-item interactions
np.random.seed(42)

# Create synthetic users and their ratings
n_users = 100
n_places = len(df_processed)

# Create user-item rating matrix
user_ids = []
place_ids = []
ratings = []

for user_id in range(1, n_users + 1):
    # Each user rates 10-30 places randomly
    n_ratings = np.random.randint(10, 31)
    rated_places = np.random.choice(df_processed.index, n_ratings, replace=False)

    for place_idx in rated_places:
        user_ids.append(user_id)
        place_ids.append(place_idx)
        # Rating influenced by actual place rating with some noise
        base_rating = df_processed.loc[place_idx, 'rating']
        synthetic_rating = np.clip(base_rating + np.random.normal(0, 0.5), 1, 5)
        ratings.append(round(synthetic_rating, 1))

# Create ratings dataframe
ratings_df = pd.DataFrame({
    'user_id': user_ids,
    'place_id': place_ids,
    'rating': ratings
})

print(f"Synthetic ratings dataset: {len(ratings_df)} ratings")
print(f"Unique users: {ratings_df['user_id'].nunique()}")
print(f"Unique places: {ratings_df['place_id'].nunique()}")
print(f"Rating distribution:")
print(ratings_df['rating'].value_counts().sort_index())

# Prepare data for Surprise library
reader = Reader(rating_scale=(1, 5))
surprise_data = Dataset.load_from_df(ratings_df[['user_id', 'place_id', 'rating']], reader)

print("\n3. SPLITTING DATA")
print("-" * 40)

# Split data for collaborative filtering
trainset, testset = surprise_train_test_split(surprise_data, test_size=0.2, random_state=42)
print(f"Training set: {trainset.n_ratings} ratings")
print(f"Test set: {len(testset)} ratings")

"""# 5. MODEL DEVELOPMENT - CONTENT BASED FILTERING"""

print("\n\n" + "="*60)
print("CONTENT-BASED FILTERING MODEL")
print("="*60)

def get_content_recommendations(place_name, cosine_sim=cosine_sim, df=df_processed, top_n=10):
    """
    Get recommendations based on content similarity
    """
    # Find the index of the place
    try:
        idx = df[df['nama'].str.lower() == place_name.lower()].index[0]
    except IndexError:
        print(f"Place '{place_name}' not found!")
        return None

    # Get similarity scores for all places
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort places by similarity score
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get top N similar places (excluding the place itself)
    sim_scores = sim_scores[1:top_n+1]

    # Get place indices
    place_indices = [i[0] for i in sim_scores]

    # Return recommended places
    recommendations = df.iloc[place_indices][['nama', 'alamat', 'rating', 'jumlah_review', 'kategori_utama', 'provinsi']].copy()
    recommendations['similarity_score'] = [score[1] for score in sim_scores]

    return recommendations

print("\n1. TESTING CONTENT-BASED FILTERING")
print("-" * 40)

# Test with first place in dataset
test_place = df_processed.iloc[0]['nama']
print(f"Mencari rekomendasi untuk: {test_place}")

content_recommendations = get_content_recommendations(test_place, top_n=5)
if content_recommendations is not None:
    print("\nTop 5 Rekomendasi (Content-Based):")
    print("-" * 40)
    for idx, row in content_recommendations.iterrows():
        print(f"{row['nama']}")
        print(f"  Lokasi: {row['alamat']}, {row['provinsi']}")
        print(f"  Rating: {row['rating']}, Reviews: {row['jumlah_review']}")
        print(f"  Kategori: {row['kategori_utama']}")
        print(f"  Similarity: {row['similarity_score']:.3f}")
        print()

"""# 6. MODEL DEVELOPMENT - COLLABORATIVE FILTERING"""

print("\n\n" + "="*60)
print("COLLABORATIVE FILTERING MODEL")
print("="*60)

print("\n1. TRAINING SVD MODEL")
print("-" * 40)

# Train SVD model
svd_model = SVD(n_factors=50, n_epochs=20, random_state=42)
svd_model.fit(trainset)

print("SVD model trained successfully!")

def get_collaborative_recommendations(user_id, model=svd_model, df=df_processed, ratings_df=ratings_df, top_n=10):
    """
    Get recommendations for a user using collaborative filtering
    """
    # Get all places that user hasn't rated
    user_ratings = ratings_df[ratings_df['user_id'] == user_id]['place_id'].values
    all_place_ids = df.index.values
    unrated_places = [place_id for place_id in all_place_ids if place_id not in user_ratings]

    # Predict ratings for unrated places
    predictions = []
    for place_id in unrated_places:
        pred = model.predict(user_id, place_id)
        predictions.append((place_id, pred.est))

    # Sort by predicted rating
    predictions.sort(key=lambda x: x[1], reverse=True)

    # Get top N recommendations
    top_predictions = predictions[:top_n]

    # Create recommendations dataframe
    rec_place_ids = [pred[0] for pred in top_predictions]
    rec_ratings = [pred[1] for pred in top_predictions]

    recommendations = df.iloc[rec_place_ids][['nama', 'alamat', 'rating', 'jumlah_review', 'kategori_utama', 'provinsi']].copy()
    recommendations['predicted_rating'] = rec_ratings

    return recommendations

print("\n2. TESTING COLLABORATIVE FILTERING")
print("-" * 40)

# Test with a random user
test_user_id = np.random.choice(ratings_df['user_id'].unique())
print(f"Mencari rekomendasi untuk User ID: {test_user_id}")

# Show user's rating history
user_history = ratings_df[ratings_df['user_id'] == test_user_id].merge(
    df_processed[['nama', 'kategori_utama']], left_on='place_id', right_index=True
)
print(f"\nRiwayat rating user (sample 5):")
print(user_history[['nama', 'kategori_utama', 'rating']].head())

collaborative_recommendations = get_collaborative_recommendations(test_user_id, top_n=5)
print(f"\nTop 5 Rekomendasi (Collaborative Filtering):")
print("-" * 40)
for idx, row in collaborative_recommendations.iterrows():
    print(f"{row['nama']}")
    print(f"  Lokasi: {row['alamat']}, {row['provinsi']}")
    print(f"  Rating Aktual: {row['rating']}, Reviews: {row['jumlah_review']}")
    print(f"  Kategori: {row['kategori_utama']}")
    print(f"  Predicted Rating: {row['predicted_rating']:.2f}")
    print()

"""# 7. EVALUATION"""

print("\n\n" + "="*60)
print("MODEL EVALUATION")
print("="*60)

print("\n1. COLLABORATIVE FILTERING EVALUATION")
print("-" * 40)

# Evaluate collaborative filtering model
predictions = svd_model.test(testset)

# Calculate RMSE and MAE
rmse = accuracy.rmse(predictions, verbose=False)
mae = accuracy.mae(predictions, verbose=False)

print(f"Root Mean Square Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")

print("\n2. CONTENT-BASED FILTERING EVALUATION")
print("-" * 40)

# For content-based filtering, we'll evaluate using diversity and coverage metrics
def evaluate_content_based_diversity(n_samples=20):
    """Evaluate diversity of content-based recommendations"""
    diversities = []

    # Sample random places for testing
    sample_places = df_processed.sample(n_samples)['nama'].values

    for place_name in sample_places:
        try:
            recs = get_content_recommendations(place_name, top_n=5)
            if recs is not None:
                # Calculate category diversity
                categories = recs['kategori_utama'].values
                unique_categories = len(set(categories))
                diversity = unique_categories / len(categories)
                diversities.append(diversity)
        except:
            continue

    return np.mean(diversities) if diversities else 0

diversity_score = evaluate_content_based_diversity()
print(f"Content-Based Diversity Score: {diversity_score:.4f}")
print("(Skor mendekati 1.0 menunjukkan rekomendasi yang beragam)")

# Coverage evaluation
def evaluate_coverage():
    """Evaluate how many items can be recommended"""
    recommendable_items = set()

    # Test with sample of places
    sample_places = df_processed.sample(min(50, len(df_processed)))['nama'].values

    for place_name in sample_places:
        try:
            recs = get_content_recommendations(place_name, top_n=10)
            if recs is not None:
                recommendable_items.update(recs.index.values)
        except:
            continue

    coverage = len(recommendable_items) / len(df_processed)
    return coverage

coverage_score = evaluate_coverage()
print(f"Content-Based Coverage Score: {coverage_score:.4f}")
print("(Skor mendekati 1.0 menunjukkan cakupan rekomendasi yang luas)")